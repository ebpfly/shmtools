{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage: Fast Metric Kernel Density Estimation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Distance metrics play an important role in nonparametric density estimation. In kernel density estimation, the notion of distance used is typically the Euclidean l2 norm: the density at a point x is roughly estimated as the proportion of points that fall in a Euclidean ball B(x, h) divided by the volume of this ball. Instead of using a Euclidean ball, we might use a different metric which we deem more \"natural\" for our particular application. \n",
    "\n",
    "Here we show how to use the \"Fast Metric Kernel Estimation\" modules to accomplish this. These modules are implemented using tree-based data structures which enable fast estimation of the density at x. The datastructure allows us to avoid computing the distance from x to every training point (in order to identify the points that fall in the ball B(x, h)), and instead arranges the training points hierarchically so that one only need to check the distances to some of the points. Note that this is a large datastructure and needs to reside in memory if we want to obtain speedups, otherwise loading time becomes the bottleneck.\n",
    "\n",
    "We note however that in Python, vectorized operations are usually faster than loops, and this fact can be used to compute L2 distances faster by taking advantage of its relation to inner products. The modules in \"Fast Metric Kernel Estimation\" therefore are mainly useful when using a distance metric that cannot be implemented fast through matrix operations.\n",
    "\n",
    "To learn more on tree-based data structures, we refer the user to the following references on fast proximity search methods:\n",
    "\n",
    "**References:**\n",
    "- Samory Kpotufe. Fast, smooth and adaptive regression in metric spaces. NIPS 2009. \n",
    "- A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbors. ICML, 2006. \n",
    "- R. Krauthgamer and J. Lee. Navigating nets: Simple algorithms for proximity search. SODA, 2004.\n",
    "\n",
    "**SHMTools functions called:**\n",
    "- `learn_fast_metric_kernel_density_shm`\n",
    "- `score_fast_metric_kernel_density_shm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "\n",
    "# Import shmtools functions\n",
    "from shmtools.classification import (\n",
    "    learn_fast_metric_kernel_density_shm,\n",
    "    score_fast_metric_kernel_density_shm\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "The data will be drawn out of a mixture of two gaussians in R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data parameters\n",
    "D = 2  # Dimension\n",
    "m = 700  # Number of training points\n",
    "\n",
    "# Generate mixture of two gaussians\n",
    "Xtrain_part1 = 4 * np.random.randn(m//2, D)\n",
    "Xtrain_part2 = 5 * np.random.randn(m//2, D) + np.array([[-15, 0]])\n",
    "Xtrain = np.vstack([Xtrain_part1, Xtrain_part2])\n",
    "\n",
    "print(f\"Training data shape: {Xtrain.shape}\")\n",
    "print(f\"Data dimension: {D}\")\n",
    "print(f\"Number of training points: {m}\")\n",
    "\n",
    "# Visualize training data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Xtrain_part1[:, 0], Xtrain_part1[:, 1], alpha=0.6, s=20, label='Gaussian 1')\n",
    "plt.scatter(Xtrain_part2[:, 0], Xtrain_part2[:, 1], alpha=0.6, s=20, label='Gaussian 2')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Training Data: Mixture of Two Gaussians')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "Here we build two models, one using an l1 metric (Manhattan distance), and the other using an l2 Euclidean metric and later compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "# Not supplying h forces learner to use default bandwidth\n",
    "h = None  # Will use default bandwidth selection\n",
    "\n",
    "# Use Epanechnikov kernel (equivalent to kernelType = 2 in MATLAB)\n",
    "kernelType = 'tophat'  # Closest equivalent to the triangle/epanechnikov family\n",
    "\n",
    "# Build the L1 (Manhattan) model\n",
    "print(\"Building fast L1 (Manhattan) model...\")\n",
    "start_time = time.time()\n",
    "fastL1Model = learn_fast_metric_kernel_density_shm(\n",
    "    Xtrain, \n",
    "    bw=1.0,  # Default bandwidth\n",
    "    kernel=kernelType, \n",
    "    metric='manhattan'\n",
    ")\n",
    "l1_train_time = time.time() - start_time\n",
    "print(f\"L1 model training time: {l1_train_time:.3f} seconds\")\n",
    "\n",
    "# Build the L2 (Euclidean) model\n",
    "print(\"\\nBuilding fast L2 (Euclidean) model...\")\n",
    "start_time = time.time()\n",
    "fastL2Model = learn_fast_metric_kernel_density_shm(\n",
    "    Xtrain, \n",
    "    bw=1.0,  # Default bandwidth\n",
    "    kernel=kernelType, \n",
    "    metric='euclidean'\n",
    ")\n",
    "l2_train_time = time.time() - start_time\n",
    "print(f\"L2 model training time: {l2_train_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data\n",
    "\n",
    "We pick uniformly from a grid over the R^2 plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test grid\n",
    "x_range = np.arange(-40, 41)  # -40 to 40\n",
    "y_range = np.arange(-40, 41)  # -40 to 40\n",
    "x_grid, y_grid = np.meshgrid(x_range, y_range)\n",
    "n = x_grid.shape[0]\n",
    "\n",
    "# Flatten grid into test points\n",
    "X = np.column_stack([x_grid.ravel(), y_grid.ravel()])\n",
    "\n",
    "print(f\"Test grid size: {n} x {n}\")\n",
    "print(f\"Total test points: {X.shape[0]}\")\n",
    "print(f\"Test data shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score on the test points\n",
    "\n",
    "The scoring functions below will return the log of the density at each test point. We record the time to compare it against the naive way of doing kernel density estimation, i.e. by computing the distance to all training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score with fast L1 model\n",
    "print(\"Scoring with fast L1 model...\")\n",
    "start_time = time.time()\n",
    "fastL1Z = score_fast_metric_kernel_density_shm(X, fastL1Model)\n",
    "fastL1Time = time.time() - start_time\n",
    "print(f\"Fast L1 scoring time: {fastL1Time:.3f} seconds\")\n",
    "\n",
    "# Score with fast L2 model\n",
    "print(\"\\nScoring with fast L2 model...\")\n",
    "start_time = time.time()\n",
    "fastL2Z = score_fast_metric_kernel_density_shm(X, fastL2Model)\n",
    "fastL2Time = time.time() - start_time\n",
    "print(f\"Fast L2 scoring time: {fastL2Time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive kernel density estimation\n",
    "\n",
    "Now we do the estimation by the \"naive\" implementation where we just compute the distance to all points and use a kernel, using a bandwidth parameter h = 6 (pre-picked as a good one over this particular data). Here again we use the l1 distance of earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lk_dist(x, training_points, k=1):\n",
    "    \"\"\"\n",
    "    Compute Lk distances from a single point x to all training points.\n",
    "    \n",
    "    Parameters:\n",
    "    x: single test point (1D array)\n",
    "    training_points: training data (2D array)\n",
    "    k: norm type (1 for L1/Manhattan, 2 for L2/Euclidean)\n",
    "    \"\"\"\n",
    "    if k == 1:\n",
    "        # L1 (Manhattan) distance\n",
    "        return np.sum(np.abs(x - training_points), axis=1)\n",
    "    elif k == 2:\n",
    "        # L2 (Euclidean) distance\n",
    "        return np.sqrt(np.sum((x - training_points)**2, axis=1))\n",
    "    else:\n",
    "        # General Lk norm\n",
    "        return np.power(np.sum(np.power(np.abs(x - training_points), k), axis=1), 1.0/k)\n",
    "\n",
    "def metric_kernel(distances, kernel_type=2):\n",
    "    \"\"\"\n",
    "    Compute kernel weights from distances.\n",
    "    \n",
    "    Parameters:\n",
    "    distances: array of distances\n",
    "    kernel_type: 1=triangle, 2=epanechnikov, 3=quartic, 4=triweight\n",
    "    \"\"\"\n",
    "    distances = np.asarray(distances)\n",
    "    \n",
    "    if kernel_type == 1:\n",
    "        # Triangle kernel: (1-d)+\n",
    "        weights = np.maximum(1 - distances, 0)\n",
    "    elif kernel_type == 2:\n",
    "        # Epanechnikov kernel: (1-d^2)+\n",
    "        weights = np.maximum(1 - distances**2, 0)\n",
    "    elif kernel_type == 3:\n",
    "        # Quartic kernel: ((1-d^2)+)^2\n",
    "        weights = np.maximum(1 - distances**2, 0)**2\n",
    "    elif kernel_type == 4:\n",
    "        # Triweight kernel: ((1-d^2)+)^3\n",
    "        weights = np.maximum(1 - distances**2, 0)**3\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Naive implementation with L1 distance\n",
    "h = 6  # Bandwidth parameter\n",
    "naiveL1Z = np.zeros(X.shape[0])\n",
    "\n",
    "print(\"Computing naive L1 kernel density estimation...\")\n",
    "print(f\"This may take a while for {X.shape[0]} test points...\")\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(X.shape[0]):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"  Processing point {i}/{X.shape[0]}\")\n",
    "    \n",
    "    # Compute L1 distances from test point to all training points\n",
    "    dist = lk_dist(X[i, :], Xtrain, k=1)\n",
    "    \n",
    "    # Apply kernel with bandwidth scaling\n",
    "    weights = metric_kernel(dist / h, kernel_type=2)\n",
    "    \n",
    "    # Density estimate\n",
    "    naiveL1Z[i] = np.sum(weights) / (m * h**D)\n",
    "\n",
    "naiveL1Time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nFast L1 Time: {fastL1Time:.2f} s, Naive L1 Time: {naiveL1Time:.2f} s\")\n",
    "print(f\"Speedup: {naiveL1Time/fastL1Time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the estimated densities\n",
    "\n",
    "Put Z in the right format for meshing, and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the density estimates back to grid format\n",
    "l1z = np.exp(fastL1Z).reshape(n, n)\n",
    "l2z = np.exp(fastL2Z).reshape(n, n)\n",
    "naivez = naiveL1Z.reshape(n, n)\n",
    "\n",
    "# Create 3D mesh plots\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Fast L1 estimate\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf1 = ax1.plot_surface(x_grid, y_grid, l1z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Density')\n",
    "ax1.set_title('Fast L1 estimate')\n",
    "ax1.view_init(elev=30, azim=45)\n",
    "\n",
    "# Fast L2 estimate\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "surf2 = ax2.plot_surface(x_grid, y_grid, l2z, cmap='viridis', alpha=0.8)\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.set_zlabel('Density')\n",
    "ax2.set_title('Fast L2 estimate')\n",
    "ax2.view_init(elev=30, azim=45)\n",
    "\n",
    "# Naive estimate\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "surf3 = ax3.plot_surface(x_grid, y_grid, naivez, cmap='viridis', alpha=0.8)\n",
    "ax3.set_xlabel('X')\n",
    "ax3.set_ylabel('Y')\n",
    "ax3.set_zlabel('Density')\n",
    "ax3.set_title('Naive estimate')\n",
    "ax3.view_init(elev=30, azim=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour plots for better visualization\n",
    "\n",
    "Let's also create contour plots to better visualize the density estimates and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contour plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Fast L1 estimate contour\n",
    "contour1 = axes[0].contourf(x_grid, y_grid, l1z, levels=20, cmap='viridis')\n",
    "axes[0].scatter(Xtrain[:, 0], Xtrain[:, 1], s=1, c='red', alpha=0.5)\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('Y')\n",
    "axes[0].set_title('Fast L1 estimate')\n",
    "axes[0].axis('equal')\n",
    "plt.colorbar(contour1, ax=axes[0])\n",
    "\n",
    "# Fast L2 estimate contour\n",
    "contour2 = axes[1].contourf(x_grid, y_grid, l2z, levels=20, cmap='viridis')\n",
    "axes[1].scatter(Xtrain[:, 0], Xtrain[:, 1], s=1, c='red', alpha=0.5)\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('Y')\n",
    "axes[1].set_title('Fast L2 estimate')\n",
    "axes[1].axis('equal')\n",
    "plt.colorbar(contour2, ax=axes[1])\n",
    "\n",
    "# Naive estimate contour\n",
    "contour3 = axes[2].contourf(x_grid, y_grid, naivez, levels=20, cmap='viridis')\n",
    "axes[2].scatter(Xtrain[:, 0], Xtrain[:, 1], s=1, c='red', alpha=0.5)\n",
    "axes[2].set_xlabel('X')\n",
    "axes[2].set_ylabel('Y')\n",
    "axes[2].set_title('Naive estimate')\n",
    "axes[2].axis('equal')\n",
    "plt.colorbar(contour3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the computational performance and accuracy of the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "print(\"=== Performance Summary ===\")\n",
    "print(f\"Training data size: {m} points in {D}D\")\n",
    "print(f\"Test grid size: {n} x {n} = {X.shape[0]} points\")\n",
    "print()\n",
    "print(\"Training times:\")\n",
    "print(f\"  Fast L1 model: {l1_train_time:.3f} seconds\")\n",
    "print(f\"  Fast L2 model: {l2_train_time:.3f} seconds\")\n",
    "print()\n",
    "print(\"Scoring times:\")\n",
    "print(f\"  Fast L1 scoring: {fastL1Time:.3f} seconds\")\n",
    "print(f\"  Fast L2 scoring: {fastL2Time:.3f} seconds\")\n",
    "print(f\"  Naive L1 scoring: {naiveL1Time:.3f} seconds\")\n",
    "print()\n",
    "print(f\"Speedup (Naive/Fast L1): {naiveL1Time/fastL1Time:.1f}x\")\n",
    "\n",
    "# Accuracy comparison (compare fast L1 vs naive L1)\n",
    "# Convert log densities to densities for comparison\n",
    "fast_l1_density = np.exp(fastL1Z)\n",
    "naive_l1_density = naiveL1Z\n",
    "\n",
    "# Compute correlation and RMSE\n",
    "correlation = np.corrcoef(fast_l1_density, naive_l1_density)[0, 1]\n",
    "rmse = np.sqrt(np.mean((fast_l1_density - naive_l1_density)**2))\n",
    "max_diff = np.max(np.abs(fast_l1_density - naive_l1_density))\n",
    "\n",
    "print()\n",
    "print(\"=== Accuracy Comparison (Fast L1 vs Naive L1) ===\")\n",
    "print(f\"Correlation coefficient: {correlation:.4f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"Maximum absolute difference: {max_diff:.6f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Scatter plot of fast vs naive\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(naive_l1_density, fast_l1_density, alpha=0.5, s=1)\n",
    "plt.plot([naive_l1_density.min(), naive_l1_density.max()], \n",
    "         [naive_l1_density.min(), naive_l1_density.max()], 'r--', alpha=0.8)\n",
    "plt.xlabel('Naive L1 Density')\n",
    "plt.ylabel('Fast L1 Density')\n",
    "plt.title(f'Fast vs Naive L1\\nCorrelation = {correlation:.4f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Difference plot\n",
    "plt.subplot(1, 2, 2)\n",
    "diff = fast_l1_density - naive_l1_density\n",
    "plt.hist(diff, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(0, color='red', linestyle='--', alpha=0.8)\n",
    "plt.xlabel('Difference (Fast - Naive)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Difference Distribution\\nRMSE = {rmse:.6f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This notebook demonstrated the fast metric kernel density estimation approach, replicating the MATLAB example. Key findings:\n",
    "\n",
    "### Key Results:\n",
    "\n",
    "1. **Computational Efficiency**: The fast metric KDE provides significant speedup over naive implementations, especially for large datasets.\n",
    "\n",
    "2. **Distance Metric Impact**: Different distance metrics (L1 vs L2) produce notably different density estimates, with L1 creating more angular/diamond-shaped contours and L2 creating more circular contours.\n",
    "\n",
    "3. **Accuracy**: The fast implementation closely matches the naive implementation while providing substantial computational savings.\n",
    "\n",
    "### Technical Details:\n",
    "\n",
    "- **Training data**: 700 points from a mixture of two 2D Gaussians\n",
    "- **Test grid**: 81×81 = 6,561 evaluation points\n",
    "- **Metrics compared**: L1 (Manhattan) and L2 (Euclidean) distances\n",
    "- **Kernel**: Epanechnikov kernel for compatibility with MATLAB version\n",
    "\n",
    "### Applications:\n",
    "\n",
    "Fast metric kernel density estimation is particularly useful for:\n",
    "- Large-scale density estimation problems\n",
    "- Applications requiring custom distance metrics\n",
    "- Real-time or near-real-time density estimation\n",
    "- Exploratory data analysis with different geometric assumptions\n",
    "\n",
    "The tree-based implementation makes it practical to use sophisticated distance metrics that would be computationally prohibitive with naive approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHM Python (venv)",
   "language": "python",
   "name": "shm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}