{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage: Direct Use of Semi-Parametric Routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Here we show how to directly use the Semiparametric routines while bypassing the \"trainOutlierDetector\" routine.\n",
    "\n",
    "The data used in this example is from the 3-story structure. More details about the data sets can be found in 3-Story Data Sets.\n",
    "\n",
    "Requires data3SS.mat dataset.\n",
    "\n",
    "SHMTools functions called:\n",
    "- arModel_shm\n",
    "- learnGMMSemiParametricModel_shm\n",
    "- scoreGMM_shm\n",
    "\n",
    "Author: Samory Kpotufe\n",
    "\n",
    "Date Created: August 19, 2009\n",
    "\n",
    "LA-CC-14-046  \n",
    "Copyright (c) 2014, Los Alamos National Security, LLC  \n",
    "All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from shmtools.features import ar_model_shm\n",
    "from shmtools.classification import (\n",
    "    learn_gmm_semiparametric_model_shm,\n",
    "    score_gmm_shm,\n",
    "    score_gmm_semiparametric_model_shm,\n",
    "    k_medians_shm,\n",
    "    roc_shm\n",
    ")\n",
    "from examples.data import load_3story_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The data here is in the form of time series in a 3 dimensional matrix (time, sensors, instances) and also a state vector representing the various environmental conditions under which the data is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_3story_data()\n",
    "dataset = data['dataset']\n",
    "states = data['damage_states']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will break each 8192 point time series into 4, 2048 point time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = np.zeros((2048, 5, 680))\n",
    "time_data_states = np.zeros(680)\n",
    "for i in range(4):\n",
    "    start_idx = 2048 * i\n",
    "    end_idx = 2048 * (i + 1)\n",
    "    time_data[:, :, i::4] = dataset[start_idx:end_idx, :, :]\n",
    "    time_data_states[i::4] = states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract some features using your favorite function, but first pick N of the instances (each time series reading over all sensors). Each instance is then transformed into a feature vector: the returned matrix has the form (instances, features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 400\n",
    "np.random.seed(42)  # For reproducibility\n",
    "idx = np.random.permutation(time_data.shape[2])[:N]\n",
    "X_data = ar_model_shm(time_data[:, :, idx])[1]  # Get RMS residuals feature vector\n",
    "X_states = time_data_states[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set 80% of states 1:9 aside as the training data, these states correspond to undamaged readings. We'll then test on the remaining 20% of 1:9 and on the \"damaged\" states 10:17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.isin(X_states, range(1, 10))  # States 1:9\n",
    "X_undamaged = X_data[idx, :]\n",
    "n_undamaged = X_undamaged.shape[0]\n",
    "n_train = round(0.8 * n_undamaged)\n",
    "X_train = X_undamaged[:n_train, :]\n",
    "X_test = np.vstack([X_undamaged[n_train:, :], X_data[~idx, :]])\n",
    "n_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set labels for the test data, 0 corresponds to undamaged, and 1 to damaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of undamaged in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_0 = n_undamaged - n_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.concatenate([np.zeros(n_test_0), np.ones(n_test - n_test_0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model over the undamaged data\n",
    "\n",
    "The next call learns a mixture of k gaussians over the undamaged data and returns the parameters of this model in dModel. The partition function is one of those in \"SemiParametricDetectors/PartitioningAlgorithms/\" or should have the same behavior as one of those functions (including signature). The \"MMFun\" is a Mixture Model function from \"SemiParametricDetectors/ParametricMixtures\" or should have the same behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_fun = k_medians_shm\n",
    "k = 5\n",
    "d_model = learn_gmm_semiparametric_model_shm(X_train, partition_fun, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a threshold from the training data\n",
    "\n",
    "We will first obtain the \"scores\" over the training data, that is the log-likelihoods that are given by the learned distribution. Then we learn a distribution of these scores, and pick a threshold so that 90% of the training data (undamaged data) has scores above this threshold (according to the distribution of scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = score_gmm_shm(X_train, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn a normal distribution over the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = stats.norm.fit(likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pick the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.9\n",
    "threshold = stats.norm.ppf(1 - confidence, model_p[0], model_p[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the detector\n",
    "\n",
    "Now the detector consists simply of getting the distribution of scores over the test data, under the distribution learned on the undamaged training data (dModel). We simply flag a test point as \"damaged\" whenever it falls below our threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score_gmm_semiparametric_model_shm(X_test, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results contains a 1 whenever we think the point is damaged, a 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scores <= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report the detector's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err = np.sum(results != test_labels) / n_test\n",
    "false_positive_err = np.sum(results[:n_test_0] != 0) / n_test_0\n",
    "false_negative_err = np.sum(results[n_test_0:] != 1) / (n_test - n_test_0)\n",
    "print(f'\\n Total error: {total_err:.2f}\\n False Positive rate: {false_positive_err:.2f}\\n False Negative rate: {false_negative_err:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives, false_positives = roc_shm(scores, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(false_positives, true_positives)\n",
    "plt.xlabel('falsePositives')\n",
    "plt.ylabel('truePositives')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}