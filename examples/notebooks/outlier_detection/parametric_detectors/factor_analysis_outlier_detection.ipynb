{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Outlier Detection Based on Factor Analysis\n\n## Introduction\n\nThe goal of this example is to discriminate time histories from undamaged and damaged conditions based on outlier detection. The parameters from an autoregressive (AR) model are used as damage-sensitive features and a machine learning algorithm based on the factor analysis (FA) technique is used to create damage indicators (DIs) invariant for feature vectors from normal structural condition and that increase when feature vectors are from damaged structural condition.\n\nAdditionally, the receiver operating characteristic (ROC) curve is applied to evaluate the performance of the classification algorithm.\n\nData sets from **Channel 5 only** of the base-excited three story structure are used in this example. More details about the data sets can be found in the [3-Story Data Sets documentation](https://www.lanl.gov/projects/ei).\n\nThis example demonstrates:\n1. **Data Loading**: 3-story structure dataset with Channel 5 only\n2. **Feature Extraction**: AR(15) model parameters as damage-sensitive features\n3. **Train/Test Split**: Training on conditions 1-90 (undamaged), testing on all 170 conditions\n4. **Factor Analysis Modeling**: Learn FA-based outlier detection model with 2 common factors\n5. **Damage Detection**: Score test data using unique factors as damage indicators\n6. **Performance Evaluation**: ROC curve analysis for classification performance\n7. **Visualization**: Time histories, damage indicators, and ROC curves\n\n**Key Insight:**\n\nFactor analysis assumes that the observed variables (AR parameters) are linear combinations of unobserved common factors plus unique factors. In the context of SHM:\n- **Common factors**: Capture operational and environmental variations (temperature, loading conditions, etc.)\n- **Unique factors**: Capture variance not explained by common factors, including damage-related changes\n\nThe damage detection is based on the magnitude of the unique factors - undamaged conditions should have small unique factors while damaged conditions should show larger deviations.\n\n**References:**\n\nKerschen, G., Poncelet, F., & Golinval, J.-C. (2007). Physical interpretation of independent component analysis in structural dynamics. Mechanical Systems and Signal Processing, 21(4), 1561-1575.\n\n**SHMTools functions used:**\n- `ar_model_shm`\n- `learn_factor_analysis_shm`\n- `score_factor_analysis_shm`\n- `roc_shm`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport sys\nimport os\n\n# Add shmtools to path - handle different execution contexts (lesson from Phase 1-3)\ncurrent_dir = Path.cwd()\nnotebook_dir = Path(__file__).parent if '__file__' in globals() else current_dir\n\n# Try different relative paths to find shmtools\npossible_paths = [\n    notebook_dir.parent.parent.parent,  # From examples/notebooks/intermediate/\n    current_dir.parent.parent,          # From examples/notebooks/\n    current_dir,                        # From project root\n    Path('/Users/eric/repo/shm/shmtools-python')  # Absolute fallback\n]\n\nshmtools_found = False\nfor path in possible_paths:\n    if (path / 'shmtools').exists():\n        if str(path) not in sys.path:\n            sys.path.insert(0, str(path))\n        shmtools_found = True\n        print(f\"Found shmtools at: {path}\")\n        break\n\nif not shmtools_found:\n    print(\"Warning: Could not find shmtools module\")\n\nfrom shmtools.utils.data_loading import load_3story_data\nfrom shmtools.features.time_series import ar_model_shm\nfrom shmtools.classification.outlier_detection import learn_factor_analysis_shm, score_factor_analysis_shm, roc_shm\n\n# Set up plotting (lesson from Phase 1: prefer automatic layout)\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 10"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Load Raw Data\n",
    "\n",
    "Load the 3-story structure dataset and extract Channel 5 data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "data_dict = load_3story_data()\n",
    "dataset = data_dict['dataset']\n",
    "fs = data_dict['fs']\n",
    "channels = data_dict['channels']\n",
    "damage_states = data_dict['damage_states']\n",
    "\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "print(f\"Sampling frequency: {fs} Hz\")\n",
    "print(f\"Channels: {channels}\")\n",
    "print(f\"Number of damage states: {len(np.unique(damage_states))}\")\n",
    "\n",
    "# Extract Channel 5 only (index 4 in Python)\n",
    "channel_5_data = dataset[:, 4, :]  # Shape: (8192, 170)\n",
    "t, n_conditions = channel_5_data.shape\n",
    "\n",
    "print(f\"\\nChannel 5 data:\")\n",
    "print(f\"Time points: {t}\")\n",
    "print(f\"Conditions: {n_conditions}\")\n",
    "print(f\"Conditions 1-90: Undamaged baseline\")\n",
    "print(f\"Conditions 91-170: Progressive damage states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Plot Sample Time Histories\n",
    "\n",
    "Plot representative time histories from undamaged and damaged conditions to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample time histories from different damage states (following MATLAB example)\n",
    "conditions = [1, 45, 91, 135]  # MATLAB 1-based condition numbers\n",
    "condition_indices = [c - 1 for c in conditions]  # Convert to 0-based Python indices\n",
    "condition_labels = ['Undamaged (State 1)', 'Undamaged (State 45)', 'Damaged (State 91)', 'Damaged (State 135)']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (idx, label) in enumerate(zip(condition_indices, condition_labels)):\n",
    "    # Plot time history from this condition\n",
    "    time_points = np.arange(1, t + 1)\n",
    "    signal = channel_5_data[:, idx]\n",
    "    \n",
    "    axes[i].plot(time_points, signal, 'k-', linewidth=0.8)\n",
    "    axes[i].set_title(f'{label}')\n",
    "    axes[i].set_ylim([-2, 2])\n",
    "    axes[i].set_xlim([1, t])\n",
    "    axes[i].set_yticks([-2, 0, 2])\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    if i >= 2:  # Bottom row\n",
    "        axes[i].set_xlabel('Observations')\n",
    "    if i % 2 == 0:  # Left column\n",
    "        axes[i].set_ylabel('Acceleration (g)')\n",
    "\n",
    "plt.suptitle('Sample Time Histories from Channel 5', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Extraction of Damage-Sensitive Features\n",
    "\n",
    "Extraction of the AR(15) model parameters from acceleration time histories. The AR parameters capture the dynamic characteristics of the structure and serve as damage-sensitive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Reshape data for AR model: (TIME, CHANNELS, INSTANCES)\ntime_data = channel_5_data[:, np.newaxis, :]  # Shape: (8192, 1, 170)\n\n# Set AR model order\nar_order = 15\n\nprint(f\"Extracting AR({ar_order}) model parameters as features...\")\n\n# Estimation of AR Parameters\nar_parameters_fv, rmse_fv, ar_parameters, ar_residuals, ar_prediction = ar_model_shm(time_data, ar_order)\n\nprint(f\"AR parameters FV shape: {ar_parameters_fv.shape}\")\nprint(f\"RMSE shape: {rmse_fv.shape}\")\nprint(f\"AR parameters shape: {ar_parameters.shape}\")\n\n# Use AR parameters as features\nfeatures = ar_parameters_fv  # Shape: (instances, features)\nn_instances, n_features = features.shape\n\nprint(f\"\\nFeature matrix:\")\nprint(f\"Instances: {n_instances}\")\nprint(f\"Features: {n_features} (1 channel × {ar_order} AR parameters)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Prepare Training and Test Data\n",
    "\n",
    "Following the original MATLAB example:\n",
    "- **Training Data**: Conditions 1-90 (undamaged baseline states)\n",
    "- **Test Data**: All 170 conditions (both undamaged and damaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define break point between undamaged and damaged conditions\n",
    "break_point = 90  # Conditions 1-90 are undamaged, 91-170 are damaged\n",
    "\n",
    "# Training feature vectors (undamaged conditions only)\n",
    "learn_data = features[:break_point, :]\n",
    "\n",
    "# Test feature vectors (all conditions)\n",
    "score_data = features.copy()\n",
    "\n",
    "print(f\"Training data shape: {learn_data.shape}\")\n",
    "print(f\"Test data shape: {score_data.shape}\")\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"Training (undamaged): conditions 1-{break_point}\")\n",
    "print(f\"Test undamaged: conditions 1-{break_point}\")\n",
    "print(f\"Test damaged: conditions {break_point+1}-{n_instances}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Statistical Modeling for Feature Classification\n",
    "\n",
    "Factor Analysis assumes that the observed features are linear combinations of a smaller number of unobserved common factors plus unique factors:\n",
    "\n",
    "**X = Λf + u**\n",
    "\n",
    "Where:\n",
    "- **X**: Observed features (AR parameters)\n",
    "- **Λ**: Factor loadings matrix\n",
    "- **f**: Common factors (capture operational/environmental variations)\n",
    "- **u**: Unique factors (capture damage-related changes)\n",
    "\n",
    "The magnitude of the unique factors serves as the damage indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Training: Learn Factor Analysis model\nnum_factors = 2  # Number of common factors (operational/environmental variations)\nest_method = \"thomson\"  # Factor scores estimation method\n\nprint(f\"Learning Factor Analysis model from training data...\")\nprint(f\"Number of common factors: {num_factors}\")\nprint(f\"Estimation method: {est_method}\")\n\nmodel = learn_factor_analysis_shm(learn_data, num_factors=num_factors, est_method=est_method)\n\nprint(f\"\\nFactor Analysis model components:\")\nprint(f\"Factor loadings shape: {model['lambda'].shape}\")\nprint(f\"Specific variances shape: {model['psi'].shape}\")\nprint(f\"Data mean shape: {model['dataMean'].shape}\")\nprint(f\"Data std shape: {model['dataStd'].shape}\")\n\n# Display factor loadings (first few elements)\nprint(f\"\\nFactor loadings (first 5 features, both factors):\")\nprint(model['lambda'][:5, :])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Scoring: Apply Factor Analysis model to test data\nprint(\"Scoring test data...\")\nDI, unique_factors, factor_scores = score_factor_analysis_shm(score_data, model)\n\nprint(f\"Damage indicators shape: {DI.shape}\")\nprint(f\"Unique factors shape: {unique_factors.shape}\")\nprint(f\"Factor scores shape: {factor_scores.shape}\")\n\nprint(f\"\\nDamage indicators (first 10): {DI[:10]}\")\nprint(f\"Damage indicators (last 10): {DI[-10:]}\")\n\n# Display factor scores for first few instances\nprint(f\"\\nFactor scores (first 5 instances):\")\nprint(factor_scores[:5, :])"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Plot Damage Indicators\n",
    "\n",
    "Visualization of the Factor Analysis-based damage indicators showing the separation between undamaged and damaged conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DIs\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "condition_numbers = np.arange(1, n_instances + 1)\n",
    "\n",
    "# Undamaged conditions (1 to break_point)\n",
    "plt.bar(condition_numbers[:break_point], -DI[:break_point],  # Note: DI is already negative, so we negate it for display\n",
    "        color='k', alpha=0.7, label='Undamaged', width=0.8)\n",
    "\n",
    "# Damaged conditions (break_point+1 to n_instances)\n",
    "plt.bar(condition_numbers[break_point:], -DI[break_point:], \n",
    "        color='r', alpha=0.7, label='Damaged', width=0.8)\n",
    "\n",
    "plt.title('Damage Indicators (DIs) from Factor Analysis', fontsize=14, fontweight='bold')\n",
    "plt.xlabel(f'Structural Condition [Undamaged(1-{break_point}) and Damaged ({break_point+1}-{n_instances})]')\n",
    "plt.ylabel(\"DI's Amplitude\")\n",
    "plt.xlim([0, n_instances + 1])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print basic statistics\n",
    "undamaged_di = DI[:break_point]\n",
    "damaged_di = DI[break_point:]\n",
    "\n",
    "print(f\"\\nDamage Indicator Statistics:\")\n",
    "print(f\"Undamaged - Mean: {np.mean(undamaged_di):.4f}, Std: {np.std(undamaged_di):.4f}\")\n",
    "print(f\"Damaged - Mean: {np.mean(damaged_di):.4f}, Std: {np.std(damaged_di):.4f}\")\n",
    "print(f\"Separation (damaged - undamaged mean): {np.mean(damaged_di) - np.mean(undamaged_di):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Visualize Factor Analysis Components\n",
    "\n",
    "Plot the factor scores and unique factors to understand how the FA model separates common and unique variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot factor scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Factor scores for both factors\n",
    "for i in range(num_factors):\n",
    "    axes[i].bar(condition_numbers[:break_point], factor_scores[:break_point, i], \n",
    "                color='k', alpha=0.7, label='Undamaged', width=0.8)\n",
    "    axes[i].bar(condition_numbers[break_point:], factor_scores[break_point:, i], \n",
    "                color='r', alpha=0.7, label='Damaged', width=0.8)\n",
    "    \n",
    "    axes[i].set_title(f'Factor {i+1} Scores')\n",
    "    axes[i].set_xlabel('Structural Condition')\n",
    "    axes[i].set_ylabel('Factor Score')\n",
    "    axes[i].set_xlim([0, n_instances + 1])\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        axes[i].legend()\n",
    "\n",
    "plt.suptitle('Common Factor Scores (Operational/Environmental Variations)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print factor scores statistics\n",
    "print(f\"\\nFactor Scores Statistics:\")\n",
    "for i in range(num_factors):\n",
    "    undamaged_fs = factor_scores[:break_point, i]\n",
    "    damaged_fs = factor_scores[break_point:, i]\n",
    "    print(f\"Factor {i+1}:\")\n",
    "    print(f\"  Undamaged - Mean: {np.mean(undamaged_fs):.4f}, Std: {np.std(undamaged_fs):.4f}\")\n",
    "    print(f\"  Damaged - Mean: {np.mean(damaged_fs):.4f}, Std: {np.std(damaged_fs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic Curve\n",
    "\n",
    "The ROC curve is used to evaluate the performance of the Factor Analysis-based classification algorithm. Each point on the curve represents a different threshold for damage detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Flag all the instances (0=undamaged, 1=damaged)\nflag = np.zeros(n_instances, dtype=int)\nflag[break_point:] = 1  # Mark conditions break_point+1 to n_instances as damaged\n\nprint(f\"Damage state flags:\")\nprint(f\"Undamaged instances: {np.sum(flag == 0)} (conditions 1-{break_point})\")\nprint(f\"Damaged instances: {np.sum(flag == 1)} (conditions {break_point+1}-{n_instances})\")\n\n# Run ROC curve algorithm\nprint(\"\\nComputing ROC curve...\")\nTPR, FPR = roc_shm(DI, flag)  # Use original DI scores\n\nprint(f\"ROC curve computed with {len(TPR)} points\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.plot(FPR, TPR, '.-b', markersize=4, linewidth=1.5, label='Factor Analysis Classifier')\n",
    "plt.plot([0, 1], [0, 1], 'k-.', linewidth=1, label='Random Classifier')\n",
    "\n",
    "plt.title('ROC Curve for Factor Analysis Outlier Detection', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('False Alarm Rate (FPR)')\n",
    "plt.ylabel('True Detection Rate (TPR)')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xticks(np.arange(0, 1.1, 0.2))\n",
    "plt.yticks(np.arange(0, 1.1, 0.2))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Add area under curve (AUC) calculation\n",
    "auc = np.trapezoid(TPR, FPR)\n",
    "plt.text(0.6, 0.2, f'AUC = {auc:.3f}', fontsize=12, \n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC Analysis Results:\")\n",
    "print(f\"Area Under Curve (AUC): {auc:.4f}\")\n",
    "print(f\"Perfect classifier AUC: 1.000\")\n",
    "print(f\"Random classifier AUC: 0.500\")\n",
    "\n",
    "# Find optimal threshold (closest to top-left corner)\n",
    "distances = np.sqrt((1 - TPR)**2 + FPR**2)\n",
    "optimal_idx = np.argmin(distances)\n",
    "optimal_tpr = TPR[optimal_idx]\n",
    "optimal_fpr = FPR[optimal_idx]\n",
    "\n",
    "print(f\"\\nOptimal Operating Point:\")\n",
    "print(f\"True Positive Rate: {optimal_tpr:.3f}\")\n",
    "print(f\"False Positive Rate: {optimal_fpr:.3f}\")\n",
    "print(f\"Accuracy: {(optimal_tpr * np.sum(flag == 1) + (1 - optimal_fpr) * np.sum(flag == 0)) / len(flag):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This example demonstrated the complete Factor Analysis-based outlier detection workflow for structural health monitoring:\n",
    "\n",
    "1. **Data Preparation**: Successfully loaded and processed the 3-story structure dataset (Channel 5)\n",
    "2. **Feature Extraction**: Used AR(15) model parameters as damage-sensitive features\n",
    "3. **Factor Analysis Modeling**: Learned FA model with 2 common factors from undamaged training data\n",
    "4. **Damage Detection**: Applied FA scoring to all test instances using unique factors as damage indicators\n",
    "5. **Performance Evaluation**: Generated ROC curve for classification performance assessment\n",
    "\n",
    "**Key insights from Factor Analysis:**\n",
    "\n",
    "Factor Analysis provides a probabilistic interpretation of the data where:\n",
    "- **Common factors** capture shared variations across features (operational/environmental effects)\n",
    "- **Unique factors** capture feature-specific variations, including damage-related changes\n",
    "\n",
    "The approach effectively separates operational/environmental variations from damage-related changes, making it particularly suitable for SHM applications where environmental conditions vary.\n",
    "\n",
    "**Key advantages of Factor Analysis-based detection:**\n",
    "- Explicit modeling of common (environmental) vs. unique (damage) variations\n",
    "- Probabilistic framework with maximum likelihood estimation\n",
    "- Multiple factor score estimation methods (Thomson, Regression, Bartlett)\n",
    "- Interpretable factor loadings show which features are most affected by each factor\n",
    "- Effective separation of environmental and damage effects\n",
    "\n",
    "**Key differences from other methods:**\n",
    "- **vs. PCA**: Factor Analysis explicitly models noise/unique variance, while PCA focuses on total variance\n",
    "- **vs. Mahalanobis**: Uses factor structure rather than simple statistical distance\n",
    "- **vs. SVD**: Probabilistic model with explicit noise modeling rather than deterministic decomposition\n",
    "- **Factor interpretation**: Common factors represent environmental effects, unique factors represent damage\n",
    "\n",
    "**See also:**\n",
    "- [Outlier Detection based on Principal Component Analysis](../basic/pca_outlier_detection.ipynb)\n",
    "- [Outlier Detection based on Mahalanobis Distance](../basic/mahalanobis_outlier_detection.ipynb)\n",
    "- [Outlier Detection based on Singular Value Decomposition](../basic/svd_outlier_detection.ipynb)\n",
    "- [Outlier Detection based on Nonlinear Principal Component Analysis](../advanced/nlpca_outlier_detection.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}