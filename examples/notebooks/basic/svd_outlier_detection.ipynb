{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Outlier Detection Based on Singular Value Decomposition\n\n## Introduction\n\nThe goal of this example is to discriminate time histories from undamaged and damaged conditions based on outlier detection. The parameters from an autoregressive (AR) model are used as damage-sensitive features and a machine learning algorithm based on the singular value decomposition (SVD) technique is used to create damage indicators (DIs) invariant for feature vectors from normal structural condition and that increase when feature vectors are from damaged structural condition.\n\nAdditionally, the receiver operating characteristic (ROC) curve is applied to evaluate the performance of the classification algorithm. In this example, each time history of the data sets is split into four segments in order to increase the number of instances available.\n\nData sets from **Channel 5 only** of the base-excited three story structure are used in this example. More details about the data sets can be found in the [3-Story Data Sets documentation](https://www.lanl.gov/projects/ei).\n\nThis example demonstrates:\n1. **Data Loading**: 3-story structure dataset with Channel 5 only, segmented into 4 parts  \n2. **Feature Extraction**: AR(15) model parameters from time segments\n3. **Train/Test Split**: Training on first 400 instances, testing on all 680 instances\n4. **SVD Modeling**: Learn SVD-based outlier detection model from training features\n5. **Damage Detection**: Score test data and normalize with min-max scaling\n6. **Performance Evaluation**: ROC curve analysis for classification performance\n7. **Visualization**: Time histories, damage indicators, and ROC curves\n\n**References:**\n\nRuotolo, R., & Surage, C. (1999). Using SVD to Detect Damage in Structures with Different Operational Conditions. Journal of Sound and Vibration, 226(3), 425-439.\n\n**SHMTools functions used:**\n- `ar_model`\n- `learn_svd`\n- `score_svd`\n- `scale_min_max`\n- `roc`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport sys\nimport os\n\n# Add shmtools to path - handle different execution contexts (lesson from Phase 1-2)\ncurrent_dir = Path.cwd()\nnotebook_dir = Path(__file__).parent if '__file__' in globals() else current_dir\n\n# Try different relative paths to find shmtools\npossible_paths = [\n    notebook_dir.parent.parent.parent,  # From examples/notebooks/basic/\n    current_dir.parent.parent,          # From examples/notebooks/\n    current_dir,                        # From project root\n    Path('/Users/eric/repo/shm/shmtools-python')  # Absolute fallback\n]\n\nshmtools_found = False\nfor path in possible_paths:\n    if (path / 'shmtools').exists():\n        if str(path) not in sys.path:\n            sys.path.insert(0, str(path))\n        shmtools_found = True\n        print(f\"Found shmtools at: {path}\")\n        break\n\nif not shmtools_found:\n    print(\"Warning: Could not find shmtools module\")\n\nfrom shmtools.utils.data_loading import load_3story_data\nfrom shmtools.features.time_series import ar_model\nfrom shmtools.classification.outlier_detection import learn_svd, score_svd, roc\nfrom shmtools.core.preprocessing import scale_min_max_shm\n\n# Set up plotting (lesson from Phase 1: prefer automatic layout)\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data\n",
    "\n",
    "In this case each time history of the original data (Channel 5) is split into four segments. For this example, we will break each 8192 point time series into 4, 2048 point time series to increase the number of available instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "data_dict = load_3story_data()\n",
    "dataset = data_dict['dataset']\n",
    "fs = data_dict['fs']\n",
    "channels = data_dict['channels']\n",
    "damage_states = data_dict['damage_states']\n",
    "\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "print(f\"Sampling frequency: {fs} Hz\")\n",
    "print(f\"Channels: {channels}\")\n",
    "print(f\"Number of damage states: {len(np.unique(damage_states))}\")\n",
    "\n",
    "# Extract Channel 5 only (index 4 in Python)\n",
    "channel_5_data = dataset[:, 4, :]  # Shape: (8192, 170)\n",
    "t_original, n_conditions = channel_5_data.shape\n",
    "\n",
    "print(f\"\\nChannel 5 data:\")\n",
    "print(f\"Time points: {t_original}\")\n",
    "print(f\"Conditions: {n_conditions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break each 8192 point time series into 4, 2048 point time series\n",
    "break_point = 400  # Threshold for undamaged vs damaged classification\n",
    "segment_length = 2048\n",
    "n_segments = 4\n",
    "\n",
    "# Initialize segmented data: (2048 time points, 1 channel, 680 instances)\n",
    "time_data = np.zeros((segment_length, 1, n_segments * n_conditions))\n",
    "\n",
    "# Split each time series into 4 segments\n",
    "for i in range(n_segments):\n",
    "    start_idx = i * segment_length\n",
    "    end_idx = (i + 1) * segment_length\n",
    "    \n",
    "    # Every 4th index starting from i: i, i+4, i+8, ...\n",
    "    segment_indices = np.arange(i, n_segments * n_conditions, n_segments)\n",
    "    \n",
    "    time_data[:, 0, segment_indices] = channel_5_data[start_idx:end_idx, :]\n",
    "\n",
    "print(f\"Segmented data shape: {time_data.shape}\")\n",
    "print(f\"Total instances: {time_data.shape[2]}\")\n",
    "print(f\"Training instances (undamaged): 1-{break_point}\")\n",
    "print(f\"Test instances (all): 1-{time_data.shape[2]}\")\n",
    "print(f\"Damaged instances: {break_point+1}-{time_data.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Time History Segments\n",
    "\n",
    "Plot one segment of one acceleration time history from four different state conditions to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one segment from four different states (following MATLAB example)\n",
    "states = [1, 7, 10, 14]  # MATLAB 1-based state numbers\n",
    "state_indices = [(state - 1) * 10 for state in states]  # Convert to 0-based Python indices\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (state, idx) in enumerate(zip(states, state_indices)):\n",
    "    # Plot time history from this state condition\n",
    "    time_points = np.arange(1, segment_length + 1)\n",
    "    signal = time_data[:, 0, idx]\n",
    "    \n",
    "    axes[i].plot(time_points, signal, 'k-', linewidth=0.8)\n",
    "    axes[i].set_title(f'State#{state}')\n",
    "    axes[i].set_ylim([-2, 2])\n",
    "    axes[i].set_xlim([1, segment_length])\n",
    "    axes[i].set_yticks([-2, 0, 2])\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    if i >= 2:  # Bottom row\n",
    "        axes[i].set_xlabel('Observations')\n",
    "    if i % 2 == 0:  # Left column\n",
    "        axes[i].set_ylabel('Acceleration (g)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Damage-Sensitive Features\n",
    "\n",
    "Extraction of the AR(15) model parameters from the segments of acceleration time histories. The order of the model was picked from the lower-bound of the range given by the optimization methods available in this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set AR model order\nar_order = 15\n\nprint(f\"Extracting AR({ar_order}) model parameters as features...\")\n\n# Estimation of AR Parameters (we need the parameters, like Mahalanobis example)\nar_parameters_fv, rmse_fv, ar_parameters, ar_residuals, ar_prediction = ar_model(time_data, ar_order)\n\nprint(f\"AR parameters FV shape: {ar_parameters_fv.shape}\")\nprint(f\"RMSE shape: {rmse_fv.shape}\")\nprint(f\"AR parameters shape: {ar_parameters.shape}\")\n\n# Use AR parameters as features\nfeatures = ar_parameters_fv  # Shape: (instances, features)\nn_instances, n_features = features.shape\n\nprint(f\"\\nFeature matrix:\")\nprint(f\"Instances: {n_instances}\")\nprint(f\"Features: {n_features} (1 channel Ã— {ar_order} AR parameters)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training and Test Data\n",
    "\n",
    "Following the original MATLAB example:\n",
    "- **Training Data**: First 400 instances (undamaged conditions)\n",
    "- **Test Data**: All 680 instances (both undamaged and damaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training feature vectors (first break_point instances)\n",
    "learn_data = features[:break_point, :]\n",
    "\n",
    "# Test feature vectors (all instances)\n",
    "score_data = features.copy()\n",
    "\n",
    "print(f\"Training data shape: {learn_data.shape}\")\n",
    "print(f\"Test data shape: {score_data.shape}\")\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"Training (undamaged): instances 1-{break_point}\")\n",
    "print(f\"Test undamaged: instances 1-{break_point}\")\n",
    "print(f\"Test damaged: instances {break_point+1}-{n_instances}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Modeling for Feature Classification\n",
    "\n",
    "In the context of data normalization process, the SVD-based machine learning algorithm is used to create DIs invariant under feature vectors from undamaged structural conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training: Learn SVD model (with standardization)\nprint(\"Learning SVD model from training data...\")\nmodel = learn_svd(learn_data, param_stand=False)  # MATLAB example uses param_stand=0\n\nprint(f\"SVD model components:\")\nprint(f\"Training data shape: {model['X'].shape}\")\nprint(f\"Singular values shape: {model['S'].shape}\")\nprint(f\"Standardization: {'Yes' if model['dataMean'] is not None else 'No'}\")\n\n# Scoring: Apply SVD model to test data\nprint(\"\\nScoring test data...\")\nDI, residuals = score_svd(score_data, model)\n\nprint(f\"Damage indicators shape: {DI.shape}\")\nprint(f\"Residuals shape: {residuals.shape}\")\nprint(f\"\\nDamage indicators (first 10): {DI[:10]}\")\nprint(f\"Damage indicators (last 10): {DI[-10:]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Normalization procedure: Scale to [0,1] range\nprint(\"Normalizing damage indicators...\")\nDI_normalized = scale_min_max_shm(-DI, scaling_dimension=1, scale_range=(0, 1))\n\nprint(f\"Original DI range: [{np.min(DI):.4f}, {np.max(DI):.4f}]\")\nprint(f\"Normalized DI range: [{np.min(DI_normalized):.4f}, {np.max(DI_normalized):.4f}]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Damage Indicators\n",
    "\n",
    "Visualization of the SVD-based damage indicators showing the separation between undamaged and damaged conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DIs\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "instance_numbers = np.arange(1, n_instances + 1)\n",
    "\n",
    "# Undamaged conditions (1 to break_point)\n",
    "plt.bar(instance_numbers[:break_point], DI_normalized[:break_point], \n",
    "        color='k', alpha=0.7, label='Undamaged')\n",
    "\n",
    "# Damaged conditions (break_point+1 to n_instances)\n",
    "plt.bar(instance_numbers[break_point:], DI_normalized[break_point:], \n",
    "        color='r', alpha=0.7, label='Damaged')\n",
    "\n",
    "plt.title('Damage Indicators (DIs) from the Test Data')\n",
    "plt.xlabel(f'Structural Condition [Undamaged(1-{break_point}) and Damaged ({break_point+1}-{n_instances})]')\n",
    "plt.ylabel(\"DI's Amplitude\")\n",
    "plt.xlim([0, n_instances + 1])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print basic statistics\n",
    "undamaged_di = DI_normalized[:break_point]\n",
    "damaged_di = DI_normalized[break_point:]\n",
    "\n",
    "print(f\"\\nDamage Indicator Statistics:\")\n",
    "print(f\"Undamaged - Mean: {np.mean(undamaged_di):.4f}, Std: {np.std(undamaged_di):.4f}\")\n",
    "print(f\"Damaged - Mean: {np.mean(damaged_di):.4f}, Std: {np.std(damaged_di):.4f}\")\n",
    "print(f\"Separation (damaged - undamaged mean): {np.mean(damaged_di) - np.mean(undamaged_di):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic Curve\n",
    "\n",
    "The ROC curve is used to evaluate the performance of the SVD-based classification algorithm. Each point on the curve represents a different threshold for damage detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Flag all the instances (0=undamaged, 1=damaged)\nflag = np.zeros(n_instances, dtype=int)\nflag[break_point:] = 1  # Mark instances break_point+1 to n_instances as damaged\n\nprint(f\"Damage state flags:\")\nprint(f\"Undamaged instances: {np.sum(flag == 0)} (indices 1-{break_point})\")\nprint(f\"Damaged instances: {np.sum(flag == 1)} (indices {break_point+1}-{n_instances})\")\n\n# Run ROC curve algorithm\nprint(\"\\nComputing ROC curve...\")\nTPR, FPR = roc(DI, flag)  # Use original DI (not normalized)\n\nprint(f\"ROC curve computed with {len(TPR)} points\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot ROC curve\nplt.figure(figsize=(8, 8))\n\nplt.plot(FPR, TPR, '.-b', markersize=4, linewidth=1.5, label='SVD Classifier')\nplt.plot([0, 1], [0, 1], 'k-.', linewidth=1, label='Random Classifier')\n\nplt.title('ROC Curve for the Test Data')\nplt.xlabel('False Alarm - FPR')\nplt.ylabel('True Detection - TPR')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.xticks(np.arange(0, 1.1, 0.2))\nplt.yticks(np.arange(0, 1.1, 0.2))\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# Add area under curve (AUC) calculation\nauc = np.trapezoid(TPR, FPR)\nplt.text(0.6, 0.2, f'AUC = {auc:.3f}', fontsize=12, \n         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nROC Analysis Results:\")\nprint(f\"Area Under Curve (AUC): {auc:.4f}\")\nprint(f\"Perfect classifier AUC: 1.000\")\nprint(f\"Random classifier AUC: 0.500\")\n\n# Find optimal threshold (closest to top-left corner)\ndistances = np.sqrt((1 - TPR)**2 + FPR**2)\noptimal_idx = np.argmin(distances)\noptimal_tpr = TPR[optimal_idx]\noptimal_fpr = FPR[optimal_idx]\n\nprint(f\"\\nOptimal Operating Point:\")\nprint(f\"True Positive Rate: {optimal_tpr:.3f}\")\nprint(f\"False Positive Rate: {optimal_fpr:.3f}\")\nprint(f\"Accuracy: {(optimal_tpr * np.sum(flag == 1) + (1 - optimal_fpr) * np.sum(flag == 0)) / len(flag):.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This example demonstrated the complete SVD-based outlier detection workflow for structural health monitoring:\n",
    "\n",
    "1. **Data Preparation**: Successfully loaded and segmented the 3-story structure dataset (Channel 5)\n",
    "2. **Feature Extraction**: Used AR(15) model parameters as damage-sensitive features from time segments\n",
    "3. **SVD Modeling**: Learned SVD-based outlier detection model from undamaged training data\n",
    "4. **Damage Detection**: Applied SVD scoring and min-max normalization to all test instances\n",
    "5. **Performance Evaluation**: Generated ROC curve for classification performance assessment\n",
    "\n",
    "**Key insights from the ROC curve:**\n",
    "\n",
    "The ROC curve shows that there is no single linear threshold able to perfectly discriminate all undamaged and damaged instances when using AR(15) parameters as damage-sensitive features with the SVD-based machine learning algorithm. The diagonal line divides the ROC space into areas of good (left) or bad (right) classification performance.\n",
    "\n",
    "Note that the optimal point (no false negatives/positives) would be in the upper-left corner of the plot. The closer the curve is to the upper-left corner, the better the classifier performance.\n",
    "\n",
    "**Key advantages of SVD-based detection:**\n",
    "- Captures changes in data structure through singular value decomposition\n",
    "- Effective for detecting rank changes in feature matrices\n",
    "- Computationally efficient singular value computation\n",
    "- Provides interpretable residuals between training and test singular values\n",
    "- Works well when damage changes the underlying data structure\n",
    "\n",
    "**Key differences from other methods:**\n",
    "- **vs. PCA**: Uses singular values directly rather than reconstruction error\n",
    "- **vs. Mahalanobis**: Focuses on matrix rank changes rather than statistical distance\n",
    "- **Data segmentation**: Increases instance count through time series segmentation\n",
    "- **ROC analysis**: Provides comprehensive performance evaluation across all thresholds\n",
    "\n",
    "**See also:**\n",
    "- [Outlier Detection based on Principal Component Analysis](pca_outlier_detection.ipynb)\n",
    "- [Outlier Detection based on Mahalanobis Distance](mahalanobis_outlier_detection.ipynb)\n",
    "- [Outlier Detection based on the Factor Analysis Model](factor_analysis_outlier_detection.ipynb)\n",
    "- [Outlier Detection based on Nonlinear Principal Component Analysis](nlpca_outlier_detection.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}