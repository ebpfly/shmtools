{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHMTools Dataset Management\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides comprehensive documentation and management utilities for the SHMTools example datasets. It covers:\n",
    "\n",
    "1. **Dataset Overview**: Physical descriptions and experimental setups\n",
    "2. **Data Loading**: Standardized loading procedures and validation\n",
    "3. **Dataset Exploration**: Structure analysis and visualization\n",
    "4. **Usage Examples**: Common data access patterns for different examples\n",
    "5. **Integrity Validation**: Automated checking of dataset completeness\n",
    "\n",
    "This notebook serves as both documentation and a practical utility for working with SHMTools data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required modules and setup the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as npimport matplotlib.pyplot as plt# Import shmtools (installed package)from examples.data import (# Set up plottingplt.style.use('default')plt.rcParams['figure.figsize'] = (12, 8)plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Availability Check\n",
    "\n",
    "Check which datasets are currently available and validate their integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset availability\n",
    "print(\"Dataset Availability:\")\n",
    "print(\"=\" * 60)\n",
    "check_data_availability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive dataset summary\n",
    "print_dataset_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: 3-Story Structure (data3SS.mat)\n",
    "\n",
    "### Physical Description\n",
    "\n",
    "The primary dataset contains measurements from a 3-story aluminum frame structure designed for structural health monitoring research at Los Alamos National Laboratory.\n",
    "\n",
    "**Physical Structure:**\n",
    "- Aluminum columns (17.7\u00d72.5\u00d70.6 cm) and plates (30.5\u00d730.5\u00d72.5 cm)\n",
    "- 4-column frame design per floor (essentially 4-DOF system)\n",
    "- Sliding rails constraining motion to x-direction only\n",
    "- Suspended center column with adjustable bumper for damage simulation\n",
    "- Base isolation using rigid foam\n",
    "\n",
    "**Instrumentation:**\n",
    "- Electrodynamic shaker for base excitation (band-limited random 20-150 Hz)\n",
    "- Load cell measuring input force (2.2 mV/N sensitivity)\n",
    "- 4 accelerometers at floor centerlines (1000 mV/g sensitivity)\n",
    "- National Instruments PXI data acquisition system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine 3-story structure data\n",
    "try:\n",
    "    data_3story = load_3story_data()\n",
    "    \n",
    "    print(\"3-Story Structure Dataset:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Dataset shape: {data_3story['dataset'].shape}\")\n",
    "    print(f\"Sampling frequency: {data_3story['fs']} Hz\")\n",
    "    print(f\"Channels: {data_3story['channels']}\")\n",
    "    print(f\"Total conditions: {len(data_3story['conditions'])}\")\n",
    "    print(f\"Damage states: {len(data_3story['state_descriptions'])}\")\n",
    "    print(f\"Description: {data_3story['description']}\")\n",
    "    \n",
    "    # Show damage state descriptions\n",
    "    print(\"\\nDamage State Descriptions:\")\n",
    "    print(\"-\" * 50)\n",
    "    for state, desc in data_3story['state_descriptions'].items():\n",
    "        print(f\"State {state:2d}: {desc}\")\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"3-Story dataset not available: {e}\")\n",
    "    print(\"Please download data3SS.mat and place it in the data directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data structure if available\n",
    "if 'data_3story' in locals():\n",
    "    dataset = data_3story['dataset']\n",
    "    damage_states = data_3story['damage_states']\n",
    "    \n",
    "    # Plot damage state distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    unique_states, counts = np.unique(damage_states, return_counts=True)\n",
    "    plt.bar(unique_states, counts)\n",
    "    plt.xlabel('Damage State')\n",
    "    plt.ylabel('Number of Tests')\n",
    "    plt.title('Distribution of Test Conditions by Damage State')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot example time series from different states\n",
    "    plt.subplot(1, 2, 2)\n",
    "    t = np.arange(dataset.shape[0]) / data_3story['fs']\n",
    "    \n",
    "    # Plot baseline condition (state 1, test 1) - channel 2\n",
    "    baseline_signal = dataset[:1000, 1, 0]  # First 1000 points, channel 2, condition 1\n",
    "    plt.plot(t[:1000], baseline_signal, 'b-', label='Baseline (State 1)', alpha=0.7)\n",
    "    \n",
    "    # Plot damaged condition (state 10, test 1) - channel 2  \n",
    "    damage_signal = dataset[:1000, 1, 90]  # First 1000 points, channel 2, condition 91 (state 10)\n",
    "    plt.plot(t[:1000], damage_signal, 'r-', label='Damaged (State 10)', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Acceleration')\n",
    "    plt.title('Sample Time Series Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\nStatistical Summary (Channel 2):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Compare baseline vs damaged conditions\n",
    "    baseline_data = dataset[:, 1, :90].flatten()  # All baseline conditions, channel 2\n",
    "    damaged_data = dataset[:, 1, 90:].flatten()   # All damaged conditions, channel 2\n",
    "    \n",
    "    print(f\"Baseline - Mean: {np.mean(baseline_data):.4f}, Std: {np.std(baseline_data):.4f}\")\n",
    "    print(f\"Damaged  - Mean: {np.mean(damaged_data):.4f}, Std: {np.std(damaged_data):.4f}\")\n",
    "    print(f\"RMS Ratio (Damaged/Baseline): {np.std(damaged_data)/np.std(baseline_data):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: Condition-Based Monitoring (data_CBM.mat)\n",
    "\n",
    "### Physical Description\n",
    "\n",
    "Rotating machinery vibration data collected from the SpectraQuest Magnum Machinery Fault Simulator for bearing and gearbox fault analysis.\n",
    "\n",
    "**Test Setup:**\n",
    "- Main shaft: 3/4\" diameter steel, 28.5\" center-to-center bearing support\n",
    "- Gearbox: Hub City M2, 1.5:1 ratio, 18/27 teeth (pinion/gear)\n",
    "- Belt drive: ~1:3.71 ratio, 13\" span, 3.7 lbs tension\n",
    "- Magnetic brake: 1.9 lbs-in torsional load\n",
    "- Shaft speed: ~1000 rpm nominally constant\n",
    "\n",
    "**Fault Conditions:**\n",
    "- Ball bearing faults (roller spin)\n",
    "- Gearbox worn tooth faults\n",
    "- Baseline conditions with ball and fluid bearings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine CBM data\n",
    "try:\n",
    "    data_cbm = load_cbm_data()\n",
    "    \n",
    "    print(\"Condition-Based Monitoring Dataset:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show available variables\n",
    "    print(\"Available variables:\")\n",
    "    for key, value in data_cbm.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(f\"  {key}: {value.shape} ({value.dtype})\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Show fault state descriptions\n",
    "    if 'fault_states' in data_cbm:\n",
    "        print(\"\\nFault State Descriptions:\")\n",
    "        print(\"-\" * 50)\n",
    "        for state, desc in data_cbm['fault_states'].items():\n",
    "            print(f\"State {state}: {desc}\")\n",
    "    \n",
    "    # Show bearing fault frequencies if available\n",
    "    shaft_freq = data_cbm['shaft_speed_rpm'] / 60.0  # Convert RPM to Hz\n",
    "    print(f\"\\nBearing Fault Frequencies (Shaft = {shaft_freq:.1f} Hz):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Cage Speed: {3.048 * shaft_freq:.1f} Hz\")\n",
    "    print(f\"Outer Race: {3.048 * shaft_freq:.1f} Hz\")\n",
    "    print(f\"Inner Race: {4.95 * shaft_freq:.1f} Hz\")\n",
    "    print(f\"Ball Spin: {1.992 * shaft_freq:.1f} Hz\")\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"CBM dataset not available: {e}\")\n",
    "    print(\"Please download data_CBM.mat and place it in the data directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBM Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CBM data if available\n",
    "if 'data_cbm' in locals() and 'dataset' in data_cbm:\n",
    "    cbm_dataset = data_cbm['dataset']\n",
    "    fs = data_cbm['fs']\n",
    "    channels = data_cbm['channels']\n",
    "    \n",
    "    print(f\"CBM Dataset shape: {cbm_dataset.shape}\")\n",
    "    \n",
    "    # Plot example signals from different channels and conditions\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Time vector\n",
    "    t = np.arange(1000) / fs  # First 1000 points for visualization\n",
    "    \n",
    "    # Plot signals from each channel\n",
    "    for i, channel in enumerate(channels):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        \n",
    "        # Plot baseline condition (assuming condition 0)\n",
    "        if cbm_dataset.shape[2] > 0:\n",
    "            baseline_signal = cbm_dataset[:1000, i, 0]\n",
    "            plt.plot(t, baseline_signal, 'b-', label='Baseline', alpha=0.7)\n",
    "        \n",
    "        # Plot fault condition (assuming later condition exists) \n",
    "        if cbm_dataset.shape[2] > 64:  # If we have fault conditions\n",
    "            fault_signal = cbm_dataset[:1000, i, 64]\n",
    "            plt.plot(t, fault_signal, 'r-', label='Fault', alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.title(f'{channel}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif 'data_cbm' in locals():\n",
    "    print(\"CBM data loaded but 'dataset' variable not found.\")\n",
    "    print(\"Available variables:\", list(data_cbm.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Datasets\n",
    "\n",
    "Brief exploration of the remaining datasets used in specialized examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other datasets if available\n",
    "datasets_to_check = [\n",
    "    ('Active Sensing', load_active_sensing_data),\n",
    "    ('Sensor Diagnostic', load_sensor_diagnostic_data),\n",
    "    ('Modal OSP', load_modal_osp_data)\n",
    "]\n",
    "\n",
    "loaded_datasets = {}\n",
    "\n",
    "for name, loader_func in datasets_to_check:\n",
    "    try:\n",
    "        data = loader_func()\n",
    "        loaded_datasets[name] = data\n",
    "        \n",
    "        print(f\"\\n{name} Dataset:\")\n",
    "        print(\"=\" * (len(name) + 10))\n",
    "        \n",
    "        # Show dataset structure\n",
    "        total_size = 0\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                size_mb = value.nbytes / (1024**2)\n",
    "                total_size += size_mb\n",
    "                print(f\"  {key}: {value.shape} ({value.dtype}) - {size_mb:.2f} MB\")\n",
    "            elif isinstance(value, (list, dict)):\n",
    "                print(f\"  {key}: {type(value).__name__} (length: {len(value)})\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(f\"Total estimated size: {total_size:.2f} MB\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n{name} dataset not available.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading {name} dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Usage Examples\n",
    "\n",
    "Demonstrate common data access patterns for different types of SHMTools examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Loading data for outlier detection examples (PCA, Mahalanobis, SVD)\n",
    "print(\"Example 1: Outlier Detection Data Loading\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # This convenience function preprocesses the 3-story data for outlier detection\n",
    "    pca_data = load_example_data('pca')\n",
    "    \n",
    "    print(f\"Preprocessed signals shape: {pca_data['signals'].shape}\")\n",
    "    print(f\"Channels included: {pca_data['channels']}\")\n",
    "    print(f\"Time points (t): {pca_data['t']}\")\n",
    "    print(f\"Channels (m): {pca_data['m']}\")\n",
    "    print(f\"Conditions (n): {pca_data['n']}\")\n",
    "    \n",
    "    # Show how to split into baseline and damaged conditions\n",
    "    signals = pca_data['signals']\n",
    "    damage_states = pca_data['damage_states']\n",
    "    \n",
    "    # Extract baseline conditions (states 1-9)\n",
    "    baseline_indices = np.where(damage_states <= 9)[0]\n",
    "    damaged_indices = np.where(damage_states >= 10)[0]\n",
    "    \n",
    "    baseline_signals = signals[:, :, baseline_indices]\n",
    "    damaged_signals = signals[:, :, damaged_indices]\n",
    "    \n",
    "    print(f\"Baseline conditions: {baseline_signals.shape[2]} tests\")\n",
    "    print(f\"Damaged conditions: {damaged_signals.shape[2]} tests\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"3-story dataset required for outlier detection examples not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Accessing specific damage states\n",
    "print(\"\\nExample 2: Accessing Specific Damage States\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'pca_data' in locals():\n",
    "    damage_states = pca_data['damage_states']\n",
    "    state_descriptions = pca_data['state_descriptions']\n",
    "    signals = pca_data['signals']\n",
    "    \n",
    "    # Access specific states\n",
    "    target_states = [1, 10, 14]  # Baseline, first damage, severe damage\n",
    "    \n",
    "    for state in target_states:\n",
    "        state_indices = np.where(damage_states == state)[0]\n",
    "        state_signals = signals[:, :, state_indices]\n",
    "        \n",
    "        # Calculate RMS for each test in this state\n",
    "        rms_values = np.sqrt(np.mean(state_signals**2, axis=0))  # RMS over time\n",
    "        mean_rms = np.mean(rms_values, axis=1)  # Mean RMS across tests\n",
    "        \n",
    "        print(f\"State {state}: {state_descriptions[state]}\")\n",
    "        print(f\"  Tests: {len(state_indices)}\")\n",
    "        print(f\"  Mean RMS per channel: {mean_rms}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Training/Testing splits commonly used in examples\n",
    "print(\"Example 3: Common Training/Testing Splits\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'pca_data' in locals():\n",
    "    signals = pca_data['signals']\n",
    "    damage_states = pca_data['damage_states']\n",
    "    \n",
    "    # Common split: Use baseline conditions for training\n",
    "    baseline_indices = np.where(damage_states <= 9)[0]  # States 1-9\n",
    "    damaged_indices = np.where(damage_states >= 10)[0]  # States 10-17\n",
    "    \n",
    "    training_signals = signals[:, :, baseline_indices]\n",
    "    testing_signals = signals[:, :, np.concatenate([baseline_indices, damaged_indices])]\n",
    "    \n",
    "    # Create binary labels for testing (0=undamaged, 1=damaged)\n",
    "    test_damage_states = damage_states[np.concatenate([baseline_indices, damaged_indices])]\n",
    "    binary_labels = (test_damage_states >= 10).astype(int)\n",
    "    \n",
    "    print(f\"Training set: {training_signals.shape[2]} undamaged conditions\")\n",
    "    print(f\"Testing set: {testing_signals.shape[2]} total conditions\")\n",
    "    print(f\"  - Undamaged: {np.sum(binary_labels == 0)} tests\")\n",
    "    print(f\"  - Damaged: {np.sum(binary_labels == 1)} tests\")\n",
    "    \n",
    "    # Alternative split: Use subset of each state for training\n",
    "    print(\"\\nAlternative split (subset training):\")\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for state in range(1, 18):  # States 1-17\n",
    "        state_indices = np.where(damage_states == state)[0]\n",
    "        # Use first 7 tests for training, last 3 for testing\n",
    "        train_indices.extend(state_indices[:7])\n",
    "        test_indices.extend(state_indices[7:])\n",
    "    \n",
    "    train_indices = np.array(train_indices)\n",
    "    test_indices = np.array(test_indices)\n",
    "    \n",
    "    print(f\"Training set: {len(train_indices)} conditions from all states\")\n",
    "    print(f\"Testing set: {len(test_indices)} conditions from all states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Integrity Validation\n",
    "\n",
    "Automated validation of all datasets to ensure they're properly loaded and structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run comprehensive dataset validation\nprint(\"Dataset Integrity Validation\")\nprint(\"=\" * 50)\n\nvalidation_results = validate_dataset_integrity()\n\n# Create summary table manually (without pandas)\nprint(f\"{'Dataset':<30} {'Size (MB)':<10} {'Available':<10} {'Valid':<8} {'Errors':<8} {'Warnings':<8}\")\nprint(\"-\" * 80)\n\nfor dataset_name, result in validation_results.items():\n    dataset_info = get_available_datasets()[dataset_name]\n    \n    dataset_file = dataset_info['file']\n    size_mb = dataset_info['size_mb']\n    available = '\u2713' if result['available'] else '\u2717'\n    valid = '\u2713' if result['valid'] else '\u2717'\n    errors = len(result['errors'])\n    warnings = len(result['warnings'])\n    \n    print(f\"{dataset_file:<30} {size_mb:<10} {available:<10} {valid:<8} {errors:<8} {warnings:<8}\")\n\n# Show detailed errors/warnings if any\nprint(\"\\nDetailed Issues:\")\nprint(\"-\" * 30)\n\nissues_found = False\nfor dataset_name, result in validation_results.items():\n    if result['errors'] or result['warnings']:\n        issues_found = True\n        dataset_info = get_available_datasets()[dataset_name]\n        print(f\"\\n{dataset_info['file']}:\")\n        \n        for error in result['errors']:\n            print(f\"  ERROR: {error}\")\n        for warning in result['warnings']:\n            print(f\"  WARNING: {warning}\")\n\nif not issues_found:\n    print(\"No issues found. All available datasets are valid.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset File Information\n",
    "\n",
    "Detailed file information and download guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data directory and file information\n",
    "data_dir = get_data_dir()\n",
    "print(f\"Data Directory: {data_dir}\")\n",
    "print(f\"Directory exists: {data_dir.exists()}\")\n",
    "print()\n",
    "\n",
    "if data_dir.exists():\n",
    "    print(\"Files in data directory:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # List all .mat files\n",
    "    mat_files = list(data_dir.glob('*.mat'))\n",
    "    \n",
    "    if mat_files:\n",
    "        for mat_file in sorted(mat_files):\n",
    "            size_mb = mat_file.stat().st_size / (1024**2)\n",
    "            print(f\"  {mat_file.name:30} ({size_mb:6.2f} MB)\")\n",
    "    else:\n",
    "        print(\"  No .mat files found\")\n",
    "    \n",
    "    # List other files\n",
    "    other_files = [f for f in data_dir.iterdir() if f.is_file() and not f.name.endswith('.mat')]\n",
    "    if other_files:\n",
    "        print(\"\\nOther files:\")\n",
    "        for other_file in sorted(other_files):\n",
    "            print(f\"  {other_file.name}\")\n",
    "else:\n",
    "    print(f\"Data directory does not exist: {data_dir}\")\n",
    "    print(\"Please create the directory and download the dataset files.\")\n",
    "\n",
    "print(\"\\nDataset Download Information:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"All datasets are from the original SHMTools library (LA-CC-14-046)\")\n",
    "print(\"developed by Los Alamos National Laboratory.\")\n",
    "print(\"\")\n",
    "print(\"To obtain the datasets:\")\n",
    "print(\"1. Download from the original MATLAB SHMTools distribution\")\n",
    "print(\"2. Extract the .mat files from the Examples/ExampleData/ directory\")\n",
    "print(f\"3. Place them in: {data_dir}\")\n",
    "print(\"\")\n",
    "print(\"See the README.md file in the data directory for detailed instructions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive dataset management utilities for SHMTools Python. Key takeaways:\n",
    "\n",
    "### Available Datasets\n",
    "1. **data3SS.mat**: Primary 3-story structure dataset (25 MB)\n",
    "2. **data_CBM.mat**: Condition-based monitoring rotating machinery (54 MB)\n",
    "3. **data_example_ActiveSense.mat**: Guided wave measurements (32 MB)\n",
    "4. **dataSensorDiagnostic.mat**: Sensor health monitoring (63 KB)\n",
    "5. **data_OSPExampleModal.mat**: Modal analysis and sensor placement (50 KB)\n",
    "\n",
    "### Key Functions\n",
    "- `load_3story_data()`: Primary structural dataset with detailed metadata\n",
    "- `load_cbm_data()`: Rotating machinery with fault information\n",
    "- `load_example_data(type)`: Convenient preprocessing for specific examples\n",
    "- `validate_dataset_integrity()`: Automated validation and checking\n",
    "- `check_data_availability()`: Quick availability status\n",
    "\n",
    "### Usage Patterns\n",
    "- **Outlier Detection**: Use `load_example_data('pca')` for preprocessed 3-story data\n",
    "- **Training/Testing**: Split by damage states or use subset sampling\n",
    "- **Validation**: Run integrity checks before starting analysis\n",
    "- **Exploration**: Use metadata and descriptions for understanding structure\n",
    "\n",
    "The enhanced data loading utilities provide comprehensive documentation, metadata, and validation capabilities that simplify working with SHMTools datasets while maintaining compatibility with the original MATLAB examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}